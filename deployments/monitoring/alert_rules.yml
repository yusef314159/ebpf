# Prometheus alert rules for eBPF HTTP Tracer

groups:
- name: ebpf-tracer-alerts
  interval: 30s
  rules:
  
  # High-level service alerts
  - alert: TracerDown
    expr: up{job="ebpf-http-tracer"} == 0
    for: 1m
    labels:
      severity: critical
      service: ebpf-tracer
    annotations:
      summary: "eBPF HTTP Tracer is down"
      description: "eBPF HTTP Tracer has been down for more than 1 minute on instance {{ $labels.instance }}"
      runbook_url: "https://wiki.company.com/runbooks/ebpf-tracer-down"

  - alert: TracerHighMemoryUsage
    expr: (process_resident_memory_bytes{job="ebpf-http-tracer"} / 1024 / 1024) > 1000
    for: 5m
    labels:
      severity: warning
      service: ebpf-tracer
    annotations:
      summary: "eBPF HTTP Tracer high memory usage"
      description: "eBPF HTTP Tracer memory usage is {{ $value }}MB on instance {{ $labels.instance }}"

  - alert: TracerHighCPUUsage
    expr: rate(process_cpu_seconds_total{job="ebpf-http-tracer"}[5m]) * 100 > 80
    for: 5m
    labels:
      severity: warning
      service: ebpf-tracer
    annotations:
      summary: "eBPF HTTP Tracer high CPU usage"
      description: "eBPF HTTP Tracer CPU usage is {{ $value }}% on instance {{ $labels.instance }}"

  # HTTP traffic alerts
  - alert: HighHTTPErrorRate
    expr: rate(http_errors_total[5m]) > 10
    for: 5m
    labels:
      severity: warning
      service: http-traffic
    annotations:
      summary: "High HTTP error rate detected"
      description: "HTTP error rate is {{ $value }} errors/sec for service {{ $labels.service }}"
      runbook_url: "https://wiki.company.com/runbooks/high-error-rate"

  - alert: CriticalHTTPErrorRate
    expr: rate(http_errors_total[5m]) > 50
    for: 2m
    labels:
      severity: critical
      service: http-traffic
    annotations:
      summary: "Critical HTTP error rate detected"
      description: "HTTP error rate is {{ $value }} errors/sec for service {{ $labels.service }}"
      runbook_url: "https://wiki.company.com/runbooks/critical-error-rate"

  - alert: HighHTTPLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_histogram_bucket[5m])) > 1
    for: 3m
    labels:
      severity: warning
      service: http-traffic
    annotations:
      summary: "High HTTP latency detected"
      description: "95th percentile latency is {{ $value }}s for service {{ $labels.service }}"
      runbook_url: "https://wiki.company.com/runbooks/high-latency"

  - alert: CriticalHTTPLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_histogram_bucket[5m])) > 5
    for: 1m
    labels:
      severity: critical
      service: http-traffic
    annotations:
      summary: "Critical HTTP latency detected"
      description: "95th percentile latency is {{ $value }}s for service {{ $labels.service }}"
      runbook_url: "https://wiki.company.com/runbooks/critical-latency"

  - alert: HTTPTrafficDrop
    expr: rate(http_requests_total[5m]) < 1
    for: 10m
    labels:
      severity: warning
      service: http-traffic
    annotations:
      summary: "HTTP traffic drop detected"
      description: "HTTP request rate is {{ $value }} requests/sec for service {{ $labels.service }}"

  # Network traffic alerts
  - alert: HighNetworkErrorRate
    expr: rate(network_errors_total[5m]) > 5
    for: 5m
    labels:
      severity: warning
      service: network-traffic
    annotations:
      summary: "High network error rate detected"
      description: "Network error rate is {{ $value }} errors/sec"

  - alert: HighNetworkTraffic
    expr: rate(network_bytes_total[5m]) > 100000000  # 100MB/s
    for: 5m
    labels:
      severity: warning
      service: network-traffic
    annotations:
      summary: "High network traffic detected"
      description: "Network traffic is {{ $value }} bytes/sec"

  # Analytics engine alerts
  - alert: AnalyticsBufferFull
    expr: analytics_buffer_usage_percent > 90
    for: 2m
    labels:
      severity: warning
      service: analytics
    annotations:
      summary: "Analytics buffer nearly full"
      description: "Analytics buffer usage is {{ $value }}%"

  - alert: AnalyticsProcessingLag
    expr: analytics_processing_lag_seconds > 30
    for: 5m
    labels:
      severity: warning
      service: analytics
    annotations:
      summary: "Analytics processing lag detected"
      description: "Analytics processing lag is {{ $value }} seconds"

  # Distributed tracing alerts
  - alert: TracingSpanDrops
    expr: rate(tracing_spans_dropped_total[5m]) > 10
    for: 5m
    labels:
      severity: warning
      service: tracing
    annotations:
      summary: "High span drop rate detected"
      description: "Span drop rate is {{ $value }} spans/sec"

  - alert: JaegerDown
    expr: up{job="jaeger"} == 0
    for: 2m
    labels:
      severity: warning
      service: jaeger
    annotations:
      summary: "Jaeger is down"
      description: "Jaeger has been down for more than 2 minutes"

- name: system-alerts
  interval: 30s
  rules:
  
  # System resource alerts
  - alert: HighDiskUsage
    expr: (node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes * 100 > 85
    for: 5m
    labels:
      severity: warning
      service: system
    annotations:
      summary: "High disk usage detected"
      description: "Disk usage is {{ $value }}% on {{ $labels.device }}"

  - alert: HighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
    for: 5m
    labels:
      severity: warning
      service: system
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is {{ $value }}%"

  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 5m
    labels:
      severity: warning
      service: system
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

- name: kubernetes-alerts
  interval: 30s
  rules:
  
  # Kubernetes-specific alerts
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "Pod is crash looping"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"

  - alert: PodNotReady
    expr: kube_pod_status_ready{condition="false"} == 1
    for: 10m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "Pod not ready"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for more than 10 minutes"

  - alert: DaemonSetNotScheduled
    expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
    for: 10m
    labels:
      severity: warning
      service: kubernetes
    annotations:
      summary: "DaemonSet not fully scheduled"
      description: "DaemonSet {{ $labels.daemonset }} in namespace {{ $labels.namespace }} is not fully scheduled"
